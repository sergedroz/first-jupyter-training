{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34c0c54-668a-4d5f-9b84-40c9b2e3a7c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T09:47:30.176163Z",
     "iopub.status.busy": "2022-07-12T09:47:30.175439Z",
     "iopub.status.idle": "2022-07-12T09:47:30.191093Z",
     "shell.execute_reply": "2022-07-12T09:47:30.186534Z",
     "shell.execute_reply.started": "2022-07-12T09:47:30.176124Z"
    },
    "tags": []
   },
   "source": [
    "<img src=\"docs/first.png\" align=\"right\">  \n",
    "\n",
    "<p></p>\n",
    "<h1>Introduction to Jupyter</h1>\n",
    "\n",
    "by Serge Droz and Eireann Leverett \n",
    "\n",
    "A lot of incident response work is about analyzing and enriching data. Some of this will be done on a daily basis, others is incident specific.\n",
    "\n",
    "Jupyter notebooks are a powerful tool helping incident responders to to develop new analysis methods, investigate data and annotate the work. Jupyter notebooks can be shared, or even worked on collaboratively if run on a Jupyter Hub. \n",
    "\n",
    "This notebook gives an introduction to Jupyter as well as to some tools useful in incident response.\n",
    "It assumes you do have a basic understanding of python. There are many [tutorials](https://wiki.python.org/moin/BeginnersGuidehttps://wiki.python.org/moin/BeginnersGuide) available online.\n",
    "\n",
    "#### Copyright\n",
    "\n",
    "This material is available under the [Creative Commons BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.\n",
    "\n",
    "## Initialization \n",
    "\n",
    "Please run the cell below by **putting the mouse into the cell** and then **pressing *shift enter***. It will initialize things. If this runs in jupyterlite it may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583248c0-1453-402c-9faa-538995b8facb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some intitialisation\n",
    "# If we run in jupyterlit we need to install dnspython. Since jupyterlite does not support sockets the fuctionality will be reduced. \n",
    "try:\n",
    "    import piplite\n",
    "    await piplite.install('dnspython')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9807595-f076-4d53-a3f1-250b7404cda8",
   "metadata": {},
   "source": [
    "<h2>Jupyter and JupyterLab</h2>\n",
    "\n",
    "[Jupyter](https://jupyter.org/) developed out of the the IPython project. It allows people to interactively explore data and store the analysis in notebooks. Out of this came JupyterLab which can be called an IDE for Jupyter Notebooks. \n",
    "\n",
    "JupyterLab supports different languages. In this training we will focus on Python. \n",
    "\n",
    "<h3>JupyterLab</h3>\n",
    "\n",
    "Familiarize yourself with the GUI. \n",
    "\n",
    "<b>Assignment</b>: Turn on line numbers\n",
    "\n",
    "<h2>First steps</h2>\n",
    "\n",
    "In this section we get familiar with Jupyter Notebooks. Each cell can either me *Markdown* (like this one), or *Code* (like the next one). We don't use the other states in this course. \n",
    "\n",
    "Markdown cells are used to document and can contain LaTeX formulas like\n",
    "\n",
    "$R_{S} = \\frac{2 M g }{c^2}$.\n",
    "\n",
    "Code cells contain code. To execute a cell, enter it and press `Shift` + `Return`\n",
    "\n",
    "Try this now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e165e-9943-4620-982d-6196e6edf189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(1+1) * 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c656528b-2c57-474f-8a67-db49df893828",
   "metadata": {},
   "source": [
    "### Some Jupyter and Jupyterlab magic\n",
    "\n",
    "There are a couple of nice features when working in a Jupyter environment. \n",
    "\n",
    "#### Old output\n",
    "\n",
    "The output of the last cell executed is contained in the variable `_`. \n",
    "This is very useful if you forgot to somehow save the result of a long calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e7847-cdf8-457e-87ae-ea87c66339b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f040b-d706-4131-81a9-f9a05ddabc82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T06:21:09.306505Z",
     "iopub.status.busy": "2022-07-14T06:21:09.305671Z",
     "iopub.status.idle": "2022-07-14T06:21:09.335855Z",
     "shell.execute_reply": "2022-07-14T06:21:09.332936Z",
     "shell.execute_reply.started": "2022-07-14T06:21:09.306458Z"
    }
   },
   "source": [
    "Jupyter extends this to allow you to access to access the output of a *cell* X type `_X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e11644-4f3c-4478-a166-b58f6c0afab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd8c25-77db-4122-bdfb-9c4d703a6609",
   "metadata": {},
   "source": [
    "You can also get the content of the cell itself by executing `_iX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd3f4d-b0bc-485a-9d86-2224f6aba6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_i5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff5eb7-8ddb-4679-8696-4afff61a6231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T06:26:33.314969Z",
     "iopub.status.busy": "2022-07-14T06:26:33.314606Z",
     "iopub.status.idle": "2022-07-14T06:26:33.328088Z",
     "shell.execute_reply": "2022-07-14T06:26:33.323989Z",
     "shell.execute_reply.started": "2022-07-14T06:26:33.314946Z"
    },
    "tags": []
   },
   "source": [
    "#### Magic commands\n",
    "\n",
    "Jupyterlab, or rather the underlying defines a couple of [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html) that allow you manipulate settings or execute commands outside Jupyter. \n",
    "These magic commands start with a `%` sign.\n",
    "\n",
    "Here are a couple of file commands: `%pwd`, `%ls` or `%cd path`. Try them out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a08568-ae10-4a02-b597-d41b9b6a511b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a01f78-a218-46a2-b488-4f33ad13ca0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T06:44:44.287162Z",
     "iopub.status.busy": "2022-07-14T06:44:44.286739Z",
     "iopub.status.idle": "2022-07-14T06:44:44.313471Z",
     "shell.execute_reply": "2022-07-14T06:44:44.308064Z",
     "shell.execute_reply.started": "2022-07-14T06:44:44.287134Z"
    },
    "tags": []
   },
   "source": [
    "# A gentle intorduction into Data Analysis for incident response\n",
    "\n",
    "Investigating an incident is quite often making sense of large amounts of data, in particular logs. Typically, after a breach happened incident responders will want to get a quick idea of what happened, and how the breach can be detected in the logs.\n",
    "\n",
    "Afterwards, logs can be systematically searched for these signs, or other IOCs. Especially in large networks this can help to quickly get an overview and identify affected systems. \n",
    "\n",
    "The P in APT stands for persistence: It's thus often important to observe the situation for an extended period of time to ensure intruders have no longer any access to the networks. Monitoring for known artefacts in logs helps, but is tedious unless it's kind of automated. \n",
    "\n",
    "## Preparing data\n",
    "\n",
    "Most data, and that is true for log data too, is typically not ready to be processed. It needs to cleaned and possibly enriched. Often logs are in a format that is not easily machine readable or suitable for the tools available. Thus data has to be cleaned.\n",
    "\n",
    "### Case study: webserver logs\n",
    "\n",
    "In the following we will analyze webserver logs. These particular logs come from a honey-pot üçØ, but the ideas apply to all kinds of logs. \n",
    "\n",
    "The log format, per the servers config is:\n",
    "\n",
    "```\n",
    "$remote_addr - $remote_user [$time_local] \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_agent\" \"$http_x_forwarded_for\" $server_port $host;\n",
    "```\n",
    "\n",
    "\n",
    "An example entry is:\n",
    "\n",
    "```\n",
    "8.26.182.72 - - [15/Jul/2022:04:44:54 +0200] \"GET / HTTP/1.1\" 200 3167 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWeb\n",
    "Kit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\" \"-\" 443 212.237.24.77\n",
    "```\n",
    "\n",
    "**Assignment**: Write a regular expression that parses this line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d3b8e-5c26-4727-a83f-6a16b33c02b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "line = '8.26.182.72 - - [15/Jul/2022:04:44:54 +0200] \"GET / HTTP/1.1\" 200 3167 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\" \"-\" 443 212.237.24.77'\n",
    "\n",
    "parse_log_line = re.compile( r'Your regexp here' )\n",
    "\n",
    "parse_log_line.findall( line )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bfc0e-32b9-4d30-8ed9-5ff9904635a8",
   "metadata": {},
   "source": [
    "Let us check if this works on different lines. The file `sample-log-01.txt` contains some logs. We'll read it and try to look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f827e2-0c71-4aef-84e4-55df9bc195f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_logs( filename ):\n",
    "    \"\"\"Read in a ngnix log and split it into fileds\"\"\"\n",
    "    with open( filename ) as fp:\n",
    "        for i, line in enumerate(fp.read().split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            if  len ( parse_log_line.findall( line ) ): \n",
    "                print( \"Line {:2d} ok\".format(i))\n",
    "            else:\n",
    "                print( \"Line {:2d} error\".format(i))\n",
    "                \n",
    "read_logs( 'data/sample-log-01.txt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b816bf-9c92-4b08-b9f7-a8671515c9c6",
   "metadata": {},
   "source": [
    "Writing regular expressions is tricky, but it is often the only way to poroperly parse logs. One that seems to work is given below. But feel free to use your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789d414-c98f-4e9d-81ca-40dcbe0ad4bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parse_log_line = re.compile( r'([0-9a-f:\\.]+) - ([^\\s]+) \\[([^\\]]+)] \"([^\\\"]*)\" (\\d+) (\\d+) \"([^\\\"]*)\" \"([^\\\"]*)\" \"([^\\\"]*)\"\\s*(\\d*)\\s*([^\\s]*)$'  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc362d-7e03-4c3a-86b3-55ec49d39e99",
   "metadata": {},
   "source": [
    "## Pandas and Dataframes\n",
    "\n",
    "The naive way of processing data is to read it into an array of arrays and the get working on it. Many scripting languages, and in particular python is not very efficient with this when processing large amounts of data. This has several reasons:\n",
    "\n",
    " - Python is a scripting language, code is translated to byte code which then is executed. While quite efficient it is no where as fast as native C implementations.\n",
    " - In python arrays are lists of pointers to the entries, because you can add whatever you want. This introduces a lot of overhead.\n",
    " \n",
    " This is not a new issue. To address it [NumPy](https://numpy.org/) was introduce in 2005 and introduce C based arrays. Based on NumPy in 2009 [Pandas](https://pandas.pydata.org/) was created to generalize this to, what's called *data frames*.\n",
    " \n",
    "In this training we will leverage the power of Pandas and give you an idea of what you can do. But it is not about Pandas, it's about using such tools in your incident response work. \n",
    "\n",
    "So let us first adapt the old `read_logs` function to actually read and store the logs. \n",
    "\n",
    "Note, that on line 12, we append \\*tmp, as our regex returns \n",
    "\n",
    "`[(ip, ....)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299bb53f-227c-493b-97a9-bf9878736cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_logs( filename ):\n",
    "    \"\"\"Read in a ngnix log and split it into fileds.\n",
    "       filname: the name of the logfile\n",
    "       Returns: an list of lists containing the logs\"\"\"\n",
    "    data = []\n",
    "    with open( filename ) as fp:\n",
    "        for i, line in enumerate(fp.read().split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            tmp = parse_log_line.findall( line ) \n",
    "            if  len ( tmp ): \n",
    "                data.append( *tmp )\n",
    "            else:\n",
    "                print( \"Error parsing line {:2d}\".format(i))\n",
    "    return data\n",
    "\n",
    "data = read_logs( 'data/sample-log-01.txt' )\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61e346-d360-45c2-9ca6-c3a3ff2808f9",
   "metadata": {},
   "source": [
    "As you can see this is not very user friendly, let's create a pandas data frame out of this. By convention data frames are named df, but you can, of course, choose any name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01339ba-3787-4c80-963e-3f3c9e7123cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame( read_logs( 'data/sample-log-01.txt' ) )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1350c-ef05-4549-9d3e-f8250a4374f0",
   "metadata": {},
   "source": [
    "This already looks better. But we are missing column headers. But they are easily supplied. Pandas can ingest a variety of data formats, `json`, `csv`, etc. form which it infers the column names. Since log files don't have this info we provide it manually. To get the syntax you can type the function and then get it's description by pressing `Ctrl-Tab`. Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0f86a-4352-4862-9c2c-baf875895774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame( read_logs( 'data/sample-log-01.txt' ) # Type Ctrl-Tab here )\n",
    "                 )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3a719-aa9c-49d2-8833-603118a7d4b3",
   "metadata": {},
   "source": [
    "Correct, it's the columns key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c89fff-b81c-4321-b2ff-cb25c8d8552c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns= [ \"IP\", 'User', 'Time', 'Request', 'Status', 'Body Bytes', 'Referer', 'User-Agent', 'Forwarded', 'Dest-Port', 'Host']\n",
    "\n",
    "df = pd.DataFrame( read_logs( 'data/sample-log-01.txt' ), columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c302a-f968-42bc-a661-23e7d6a2bb0f",
   "metadata": {},
   "source": [
    "This looks already much nicer, but we can do better. However all entries are strings. You can specify the data type directly on a per column basis when importing a csv, but not here. We have to apply this conversion later. This is done by operating on an entire columns. This is very efficient and is kind of the philosophy behind pandas. rather than iteration on individual cells you operate on rows or columns. Since Pandas is written in C this is a lot faster.\n",
    "\n",
    "There are many ways of doing this. A fairly common one is using `apply` which takes a function as argument and applies it to each value in a column (or row). Technically this returns a `Series`, nut think of a new column or row. You can assign this result either to a new column or overwrite an existing one. Here we do the latter. \n",
    "\n",
    "We first convert `Status`, `Body Bytes` and `Dest-Port` to integers. We then convert the timestamp to a python `datetime` object, using the pandas built in function `pandas.to_datetime( )`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69796d37-3c16-45c9-8a5c-bb26227d4e95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame( read_logs( 'data/sample-log-01.txt' ), columns=columns )\n",
    "df['Status']     = df['Status'].apply( lambda x: int(x))\n",
    "df['Body Bytes'] = df['Body Bytes'].apply( lambda x: int(x))\n",
    "df['Dest-Port']  = df['Dest-Port'].apply( lambda x: int(x))\n",
    "df['Time'] = pd.to_datetime( df['Time'], format=\"%d/%b/%Y:%H:%M:%S %z\" , utc=True )\n",
    "df['Time'] = df['Time'].apply( lambda x: x.tz_convert('UTC').tz_localize(None) ) # Convert to UTC\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78cc94-2bed-4a51-9357-895e729b355d",
   "metadata": {},
   "source": [
    "### A note on timestamps\n",
    "\n",
    "Timestamps are crucial when analyzing logs files. One of the biggest sources of fatal mistakes are wrong time zones. It is best practice to store logs in UTC, or if this is not feasible store time zone information along. Timestamps without timezone information are useless. \n",
    "\n",
    "Here we convert all timestamps to UTC and then remove the timezone info in the note book. This will make working with date a lot easier, as there is no ambiguity. \n",
    "\n",
    "This is done with the rather clumsy expression \n",
    "```\n",
    "lambda x: x.tz_convert('UTC').tz_localize(None) \n",
    "```\n",
    "\n",
    "The important takeaways are:\n",
    " - **Ensure your timezone information is correct**\n",
    " - **Agree on a timezone to work in (we recommend UTC) and stick to it**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1ed52-af06-4eae-a02c-c7c0a51a5c47",
   "metadata": {},
   "source": [
    "Let us merge all this into one function. Developing a notebook often follows this iterative approach. One starts with a simple task, i.e. reading the log file and splitting it up into components. Then adding some further sanitation. Depending on the situation this may be quite complex. \n",
    "\n",
    "At the end you group things together. How much should go into one function is a matter of taste, but typically thins that belong together logically can go into one function. Find your own style, and stick to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2e386-0f2d-4068-aea9-36a78824e5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_logs( filename ):\n",
    "    \"\"\"Read in a ngnix log and split it into fileds.\n",
    "       filname: the name of the logfile\n",
    "       Returns: a data frame containing the log\"\"\"\n",
    "    data = []\n",
    "    with open( filename ) as fp:\n",
    "        for i, line in enumerate(fp.read().split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            tmp = parse_log_line.findall( line ) \n",
    "            if  len ( tmp ): \n",
    "                data.append( *tmp )\n",
    "            else:\n",
    "                print( \"Error parsing line {:2d}\".format(i))\n",
    "\n",
    "    columns= [ \"IP\", 'User', 'Time', 'Request', 'Status', 'Body Bytes', 'Referer', 'User-Agent', 'Forwarded', 'Dest-Port', 'Host']\n",
    "\n",
    "\n",
    "    df = pd.DataFrame( data, columns=columns )\n",
    "    df['Status']     = df['Status'].apply( lambda x: int(x))\n",
    "    df['Body Bytes'] = df['Body Bytes'].apply( lambda x: int(x))\n",
    "    df['Dest-Port']  = df['Dest-Port'].apply( lambda x: int(x))\n",
    "    df['Time'] = pd.to_datetime( df['Time'], format=\"%d/%b/%Y:%H:%M:%S %z\" , utc=True )\n",
    "    df['Time'] = df['Time'].apply( lambda x: x.tz_convert('UTC').tz_localize(None) ) # Convert to UTC\n",
    "    return df\n",
    "\n",
    "df = read_logs( 'data/sample-log-01.txt' )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbb4e0-9f97-4daa-ba2b-28704f1d3e74",
   "metadata": {},
   "source": [
    "So let's try this with a full server log. Do the above but with the file `webserver-log-01.txt`. \n",
    "\n",
    "Does your regex work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0256c-75ce-459f-9873-abf875a4cc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = read_logs( 'data/webserver-log-01.txt' )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e44c3-be40-493a-87b3-b1102a42d8fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Obviously something went wrong. Any idea what? (Click the the ... below for an answer)\n",
    "\n",
    "The problem seems to be that the column `Dest-Port` cannot be converted. The reason is, that this seems to have been added to the log file at some later point. This is quite typical: Data often is not consistent. \n",
    "\n",
    "Here the fix is simple. Replace the line\n",
    "```\n",
    "df['Dest-Port']  = df['Dest-Port'].apply( lambda x:  int(x)  )\n",
    "```\n",
    "by \n",
    "```\n",
    "df['Dest-Port']  = df['Dest-Port'].apply( lambda x:  int(x) if x else -1  )\n",
    "```\n",
    "We will thet back to this topic again with more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b82464-4c26-4281-9a0f-e8fb3c464624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_logs( filename ):\n",
    "    \"\"\"Read in a ngnix log and split it into fileds.\n",
    "       filname: the name of the logfile\n",
    "       Returns: a data frame containing the log\"\"\"\n",
    "    data = []\n",
    "    with open( filename ) as fp:\n",
    "        for i, line in enumerate(fp.read().split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            tmp = parse_log_line.findall( line ) \n",
    "            if  len ( tmp ): \n",
    "                data.append( *tmp )\n",
    "            else:\n",
    "                print( \"Error parsing line {:2d}\".format(i))\n",
    "\n",
    "    columns= [ \"IP\", 'User', 'Time', 'Request', 'Status', 'Body Bytes', 'Referer', 'User-Agent', 'Forwarded', 'Dest-Port', 'Host']\n",
    "\n",
    "\n",
    "    df = pd.DataFrame( data, columns=columns )\n",
    "    df['Status']     = df['Status'].apply( lambda x: int(x))\n",
    "    df['Body Bytes'] = df['Body Bytes'].apply( lambda x: int(x))\n",
    "    df['Dest-Port']  = df['Dest-Port'].apply( lambda x:  int(x) if x else -1  )\n",
    "    df['Time'] = pd.to_datetime( df['Time'], format=\"%d/%b/%Y:%H:%M:%S %z\" , utc=True )\n",
    "    df['Time'] = df['Time'].apply( lambda x: x.tz_convert('UTC').tz_localize(None) ) # Convert to UTC\n",
    "    return df\n",
    "df = read_logs( 'data/webserver-log-01.txt' )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd436a7-cd7a-4a0c-81d8-556957575976",
   "metadata": {},
   "source": [
    "## Investigating data\n",
    "\n",
    "Let us investigate this data a bit. Looking at new logs it's not always clear what the interesting entries are. The only way to find out is playing with the data. \n",
    "\n",
    "A very valuable strategy is looking at the distribution of values. i.e. how many different values are there. Is it something very rare? Or are there more entries than you expect? Or is there something in the middle ground that might be interesting. \n",
    "\n",
    "To start with let's look at user-agents\n",
    "\n",
    "### User-Agents - value_counts()\n",
    "\n",
    "The dataframe method `value_counts()` counts returns a pandas Series (think of a labeled array) of the counts of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42685270-737d-453f-bdf1-f2c612a01b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = df['User-Agent'].value_counts()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9707e9-f04f-4cd0-a88a-7749eee3e033",
   "metadata": {},
   "source": [
    "As usual, Pandas shows the the first and last five entries. What do you see?\n",
    "\n",
    "The first handful of entries are likely common user-agents. Let's look at the first 20 entries. We can do this by using the normal slicing notion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85800df1-5803-46c0-a2dc-9a604c1bc4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b285d8a-8cfe-400a-b258-3185fb0ad60b",
   "metadata": {},
   "source": [
    "Most User-Agents are fairly long. So let's look at entries which are short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62acb985-87f4-4027-8b7d-08e5f8b51c51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['User-Agent'].str.len() < 40 ][['IP','User-Agent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9921cfd-5b4b-4f4d-b3a2-46e7f1483f62",
   "metadata": {},
   "source": [
    "This introduces some new concepts for dataframes:\n",
    "\n",
    " 1. Selecting based on a condition. \n",
    " 2. Only displaying selected columns\n",
    " \n",
    " However let's see if we certain IPs do can be correlated to certain User-Agents.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1774689-5f5d-4b2b-b7b3-5be06591004c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['User-Agent'].str.len() < 50 ].groupby(['IP','User-Agent']).size().reset_index(name='Count').sort_values(by=['Count'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147578c0-afd2-4570-9572-025cc24b61e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is done using `groupby()` which by itself returns a `pandas.core.groupby.generic.DataFrameGroupBy object`. This object can be processed further. The `reset_index()` then creates a dataframe again, which we sort. \n",
    "\n",
    "This gives us a list of IPs that can be further investigated. \n",
    "\n",
    "Let us now look at User-Agents containing unusual characters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab7ff5-5a94-4279-8980-0fde5fe68c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['User-Agent'].str.contains('<') ]['User-Agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82e7ac-5141-4a1c-a09a-93c4fc5eb926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T08:49:25.921399Z",
     "iopub.status.busy": "2022-07-27T08:49:25.920522Z",
     "iopub.status.idle": "2022-07-27T08:49:25.936032Z",
     "shell.execute_reply": "2022-07-27T08:49:25.934417Z",
     "shell.execute_reply.started": "2022-07-27T08:49:25.921356Z"
    },
    "tags": []
   },
   "source": [
    "This clearly looks lie an attempted SQL injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87a592-1a4b-448f-84f6-ab55456bae4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['User-Agent'].str.contains('(?:<|\\$|>|\\?|\\|)') ]['User-Agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc14ae-cc27-4282-9bd1-ed58ebbdee7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T17:08:20.526321Z",
     "iopub.status.busy": "2022-07-26T17:08:20.525240Z",
     "iopub.status.idle": "2022-07-26T17:08:20.577946Z",
     "shell.execute_reply": "2022-07-26T17:08:20.560622Z",
     "shell.execute_reply.started": "2022-07-26T17:08:20.526265Z"
    }
   },
   "source": [
    "Note, that we now have supplied a regular expression `(?:<|\\$|>|\\?|\\|)`. Any idea what we see?\n",
    "\n",
    "We can no extract the IPs, as they are obviously malicious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c4b4d-bbfe-4cd7-ae8d-019d7ba0ab08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['User-Agent'].str.contains('(?:<|\\$|>|\\?|\\|)') ]['IP'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ccab1d-305b-4b17-94f6-44561a08f84a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T06:37:51.162038Z",
     "iopub.status.busy": "2022-07-27T06:37:51.161661Z",
     "iopub.status.idle": "2022-07-27T06:37:51.187240Z",
     "shell.execute_reply": "2022-07-27T06:37:51.183195Z",
     "shell.execute_reply.started": "2022-07-27T06:37:51.162005Z"
    }
   },
   "source": [
    "### requests - finding substrings\n",
    "Let us now look at the URLs or rather the requests. As before we first take a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9541a-614c-4f7f-b3c6-87e8bf3611dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Request']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e1478b-8a7e-4ba3-a8db-73a8e1aae78d",
   "metadata": {},
   "source": [
    "Just plain requests are probably not interesting, so let's exclude them. This is achieved by prepending a boolean expression by a `~`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec01871-1e83-4137-8712-e84a7e385f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[~df['Request'].str.contains('^GET / HTTP/')]['Request']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ca37-426f-4b21-80a2-6b379eed640f",
   "metadata": {},
   "source": [
    "Obviously a lot of scanning is going on. Many of the requests are very obvious, others less so. But often you can find the specific vulnerability they are scanning for. For example the expression `^GET /s/..x/_/;/(?:META|WEB)-IN` tries to exploit a bug in Jira: https://github.com/ColdFusionX/CVE-2021-26086\n",
    "\n",
    "Can you find the respectve entries?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a2aad-21e7-41e5-9c1c-f7fe6efb5287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')][['IP','Request']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6374b2-8601-48bb-9d70-cdec22e691b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')]['IP'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0a064-0831-4791-b3dc-fd2b48aac7e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T10:15:46.190483Z",
     "iopub.status.busy": "2022-07-27T10:15:46.190089Z",
     "iopub.status.idle": "2022-07-27T10:15:46.292213Z",
     "shell.execute_reply": "2022-07-27T10:15:46.290372Z",
     "shell.execute_reply.started": "2022-07-27T10:15:46.190458Z"
    },
    "tags": []
   },
   "source": [
    "The first IP resolves to `probe-nl001.rand0.leakix.org` which seems to be a security scanner (See [leakIX](https://leakix.net/). Looking at the User-Agent gives again more information. See [l9explor](https://github.com/LeakIX/l9explore). The other IPs are from cloud providers, and don't really give any further information. It's probably safe to assume they are malicious. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d64a72-b409-42fe-b816-4a5e0f476239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')][['IP','User-Agent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1451d4b-6063-4f43-96a3-9fd087cbbef2",
   "metadata": {},
   "source": [
    "Let us now look at IPv4 addresses in requests. Typically these are some form of injections. You should be able to search these entries yourself. It turns out that the output is fairly wide, but pandas tends to truncate this, which is annoying if you just paid a lot of money for a super wide screen. It's easy to tell pandas to use the full width available. \n",
    "\n",
    "Can you guess what these requests are? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0689b-9291-4a94-83de-ad5692345357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_witdh = pd.options.d['display']['max_colwidth']\n",
    "pd.options.d['display']['max_colwidth'] = 0\n",
    "display(df[df['Request'].str.contains('\\d+\\.\\d+\\.\\d+\\.\\d+')][['IP','Request']])\n",
    "pd.options.d['display']['max_colwidth'] = old_witdh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfa7e1-bd05-4ee7-aede-8f10c91c8205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-27T15:52:20.255454Z",
     "iopub.status.busy": "2022-07-27T15:52:20.255105Z",
     "iopub.status.idle": "2022-07-27T15:52:20.268225Z",
     "shell.execute_reply": "2022-07-27T15:52:20.261218Z",
     "shell.execute_reply.started": "2022-07-27T15:52:20.255430Z"
    },
    "tags": []
   },
   "source": [
    "You likely guessed it: These are Mirai infections. \n",
    "\n",
    "## Visualizing logs\n",
    "\n",
    "So far we only looked üëÄ at the data. This is great to get an idea of what there might be, but it's bad conveying the bigger picture. Jupyter is a great tool to visualize you data. Pandas itself has some visualization build in. For example let's look at the IPs trying to exploit the Jira bug above. \n",
    "\n",
    "### Simple overviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f683c-da05-4031-8299-8be7dbe0cc30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')]['IP'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7633aae-d581-453e-ae83-214bb1f60790",
   "metadata": {},
   "source": [
    "Similarly on create bar plots. This is a quick method to get an idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fd818e-18ab-4d6a-959b-e59200c8d3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')]['IP'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e26ca8-f5c6-42d7-908a-36b9f0053107",
   "metadata": {},
   "source": [
    "### Using matplotlib \n",
    "\n",
    "This gives a already a good idea, but the plot looks a bit meager. While Pandas allows some configuration, it's best to leverage the full power of the underlying matplotlib. It allows you to customize your plots a lot. To show plots inline we use the magic command `%matplotlib inline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ccdd9-e56e-45a8-9fae-d54a7f9398c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "colors = [\"#ffd700\",\"#ffb14e\",\"#fa8775\",\"#ea5f94\",\"#cd34b5\",\"#9d02d7\",\"#0000ff\"][::-1]\n",
    "fig, ax = plt.subplots( figsize=(8,8) )\n",
    "df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')]['IP'].value_counts().plot(kind='pie', ax=ax, fontsize=18, colors=colors)\n",
    "ax.set_title('IPs exploiting CVE-2021-26086', fontsize=24)\n",
    "_ = ax.set_ylabel(\"\")\n",
    "_ = ax.add_artist(plt.Circle( (0,0), 0.6, color='white'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d027fbe-af81-4e5c-9fb8-8486441945c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T17:11:58.179663Z",
     "iopub.status.busy": "2022-07-28T17:11:58.178200Z",
     "iopub.status.idle": "2022-07-28T17:11:58.199899Z",
     "shell.execute_reply": "2022-07-28T17:11:58.190116Z",
     "shell.execute_reply.started": "2022-07-28T17:11:58.179623Z"
    },
    "tags": []
   },
   "source": [
    "### Time series plots\n",
    "We can also plot the the entries over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6edf9-04fc-485d-9ce9-7cb9182d14a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = df[df['Request'].str.contains('^GET /s/..x/_/;/(?:META|WEB)-IN')]\n",
    "fig, ax = plt.subplots(figsize=(15,3))\n",
    "ax.scatter(tmp['Time'],tmp['IP'])\n",
    "plt.xticks(rotation=70, ha='right',fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax.set_title('IPs exploiting CVE-2021-26086', fontsize=18)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce0c6a-9fc8-45e3-8cc0-a3116de2d276",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T17:08:20.838113Z",
     "iopub.status.busy": "2022-07-28T17:08:20.837677Z",
     "iopub.status.idle": "2022-07-28T17:08:20.863640Z",
     "shell.execute_reply": "2022-07-28T17:08:20.856274Z",
     "shell.execute_reply.started": "2022-07-28T17:08:20.838089Z"
    },
    "tags": []
   },
   "source": [
    "While at it, let's do the same for the log4j entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8296e-c94d-49fd-9b93-ee3ff19a7bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = df[df['User-Agent'].str.contains('\\$') ]\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "ax.scatter(tmp['Time'],tmp['IP'])\n",
    "plt.xticks(rotation=70, ha='right',fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "ax.set_title('IPs exploiting log4j', fontsize=18)\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba24a37a-6f10-4e8f-9e16-f22a9a043169",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T17:10:25.793324Z",
     "iopub.status.busy": "2022-07-28T17:10:25.792093Z",
     "iopub.status.idle": "2022-07-28T17:10:25.817779Z",
     "shell.execute_reply": "2022-07-28T17:10:25.816225Z",
     "shell.execute_reply.started": "2022-07-28T17:10:25.793146Z"
    },
    "tags": []
   },
   "source": [
    "### Binning\n",
    "\n",
    "Often there is too much data to plot each individual point, or we're simply not interested in such detailed info. \n",
    "\n",
    "This is where binning is useful. Let us see if there is a daily variation of the log entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432bcd0-4bc7-43cf-8ada-dfa29aaf634b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "t_end = df.iloc[-1]['Time'].replace(hour=23, minute=59, second=59, microsecond=999999) \n",
    "t_start = t_end-datetime.timedelta(days=21)\n",
    "\n",
    "bins = [ t_end - datetime.timedelta(days=i) for i in range(3 * 7  ,-1,-1) ]\n",
    "labels = [ (t_end - datetime.timedelta(days=i)).strftime(\"%d.%m.%Y\")  for i in range(3 * 7 - 1,-1,-1) ]\n",
    "x = pd.cut(df[df['Time']>=datetime.datetime(t_start.year,t_start.month,t_start.day)]['Time'], bins, labels=labels, right=False)\n",
    "ax = x.value_counts(sort=False).plot(kind=\"bar\",figsize = (12,6))\n",
    "ax.set_title('Request per day', fontsize=18)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "_ = ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c946c1-0fb6-4f7f-a01f-77bc727b3184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-28T18:51:07.632674Z",
     "iopub.status.busy": "2022-07-28T18:51:07.631770Z",
     "iopub.status.idle": "2022-07-28T18:51:07.656035Z",
     "shell.execute_reply": "2022-07-28T18:51:07.652848Z",
     "shell.execute_reply.started": "2022-07-28T18:51:07.632621Z"
    },
    "tags": []
   },
   "source": [
    "Let us walk through this code, as it introduces some new concepts.\n",
    "\n",
    "Let's first look at the expression \n",
    "\n",
    "```\n",
    "df.iloc[-1]['Time']\n",
    "```\n",
    "\n",
    "You may have noticed that each data frame has an index. iloc allows you to access rows by index. And it supports the usual python slicing., i.e. `df.iloc[10:15]` would return rows 10-14 (five ros, starting at 10).\n",
    "\n",
    "The `-1` signifies the last row, so what we do here is get the time of the last entry, assuming that the log is sorted. If this assumption is not true we could use `max()`. But for large datasets this is slower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608bd88-b62b-4806-94ac-1eedaef88877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Time'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19656629-1965-400f-8ab5-9cd4d7a33f41",
   "metadata": {},
   "source": [
    "The `replace()` functions set this timestamp to the end of the day. \n",
    "\n",
    "In the next line we create the bins. We count down 21 days from the last day in the set. We stop at `-1` because for the binning to work we need the endpoints included, i.e. n bins require n+1 entries in this array. \n",
    "\n",
    "Next we create the labels. Notice, that there is one label less, corresponding to the number of bins. Next the actual binning happens using the pandas function `cut()`. \n",
    "\n",
    "We then do a `value_count()` on this, which we plot. Note that plot returns the \"axes\" of the plot, i.e. the object that matplotlib operates on. \n",
    "\n",
    "Don't worry if you don't get the intricate details of what is going on here, this seems quite confusing. Most of the time one just uses existing code and adjusts it to the current needs, at leas that's what the author does. \n",
    "\n",
    "This ends the first introduction you should now be able to do the basic analysis of a set of log files. In the next session we will look at enriching data.\n",
    "\n",
    "# Assignments\n",
    "\n",
    "Below are a few assignments you can try to solve. \n",
    "\n",
    "## DNS over http\n",
    "\n",
    "The log file contains entries from people sensing DNS over http queries. Can you find them? How would you go about this? Please try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba678810-54ce-42b0-a2dd-dc77b80982a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a602b93-6567-4821-a6a8-97143ebbb527",
   "metadata": {},
   "source": [
    "One solution is to search for entries containing the substring \"dns=\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c894a-a945-4042-bdba-57ba9fceb4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp=df[df['Request'].str.contains('dns=')]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba21a1d-bba7-4d92-be32-971795f265b9",
   "metadata": {},
   "source": [
    "### Advanced: Analyse the query\n",
    "\n",
    "The part after `dns=` is the base64 encoded query. Try to extract the actual query. To decode base64 use the function `base64.b64decode()`. This will return a raw dns query. You can decode this by feeding it into\n",
    "\n",
    "```\n",
    "import dns.message\n",
    "dns.message.from_wire( raw_data  )\n",
    "```\n",
    "\n",
    "Play around, this will likely take some iterations to finish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639632a-a2de-4000-ae94-ea06155c3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0941fef-1a43-4c43-8f15-9e7a3d912d49",
   "metadata": {},
   "source": [
    "What queries do you find? Is this working as expected? \n",
    "\n",
    "A working solution is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb6613-a776-4da9-882f-dabaef83aa99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import dns.message\n",
    "\n",
    "def extract_dns( x ):\n",
    "    b64query = re.findall( r'dns=(.+)\\s+',x)[0]\n",
    "    #print( b64query )\n",
    "    for p in [ '', '=', '==']:\n",
    "        try:\n",
    "            query = base64.b64decode( b64query + p )\n",
    "            return str( dns.message.from_wire( query  ) )\n",
    "        except Exception as e:\n",
    "            msg = e\n",
    "    return \"error: {} -> {}\".format(b64query, msg)\n",
    "#extract_dns( \"GET /resolve?dns=q80BAAABAAAAAAAAA3d3dwdleGFtcGxlA2NvbQAAAQAB HTTP/1.1\" )\n",
    "x = tmp['Request'].apply( extract_dns  )\n",
    "x.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d6475-8144-4008-9b34-fa4de7da9fda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-29T08:09:27.717697Z",
     "iopub.status.busy": "2022-07-29T08:09:27.716706Z",
     "iopub.status.idle": "2022-07-29T08:09:27.773552Z",
     "shell.execute_reply": "2022-07-29T08:09:27.765537Z",
     "shell.execute_reply.started": "2022-07-29T08:09:27.717651Z"
    },
    "tags": []
   },
   "source": [
    "## Analyze scanners\n",
    "\n",
    "Some scanners have a specific keyword in their User-Agent field which identifies them. A list is below. (Many are recognized by their DNS name, which we will look at in the next lesson.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca918f3d-39fc-4f59-98bb-a0c108810218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ua_scanner_keywords = [ 'project_patchwatch', 'scaninfo@expanseinc.com', 'project-resonance.com' ,\n",
    "                            'gdnplus.com', 'censys.io', 'netsystemsresearch.com', 'Nmap Scripting Engine'\n",
    "                            'internet-measurement.com', 'tchelebi.io', 'paloaltonetworks.com', 'security.ipip.net' ] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba747a0-df93-4ea4-a88c-d5458840ca03",
   "metadata": {},
   "source": [
    "Find all scanners log entries and plot a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813af56-9349-4702-a781-f620e550d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db768f-5c61-4686-a428-f48d1c14b006",
   "metadata": {},
   "source": [
    "A solution is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027733a5-1119-45ea-bc53-0874bdb72bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_scanner( x ):\n",
    "    for kw in ua_scanner_keywords:\n",
    "        if kw in x:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "tmp = df[df['User-Agent'].apply( is_scanner )]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee1ed3-ec17-4df0-b5fc-08cfc33430f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,25))\n",
    "ax.scatter(list(tmp['Time']),list(tmp['IP']), s=3 )\n",
    "#plt.xticks(rotation=70, ha='right',fontsize=12)\n",
    "plt.yticks(fontsize=6)\n",
    "ax.set_title('IPs scanning', fontsize=18)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4b243-8100-4fc0-8b76-f2ebc868fb65",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This concludes the first lesson. You should now have a basic understanding of Jupyter and pandas. Both are very complex and powerful tools, so don't despair if you don't understand certain things. You need to practice this. \n",
    "\n",
    "You also should understand that investigating an incident is an iterative process. Typically you improve your notebook for every new analysis. It's recommended to start a new investigation in a new notebook, rspt. a copy of an existing, and not reuse the old one. That way you don't accidentally delete something that used to work and you have an archive of your previous incidents. \n",
    "\n",
    "And finally: Do add ample comments, you'll be grateful for them two month in the future. \n",
    "\n",
    "### Next steps\n",
    "\n",
    "In the next lesson we will look at enriching data with external information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247bc66-11ed-46cd-928f-5ef816a81c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
