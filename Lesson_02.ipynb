{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a51b8ac-b4d8-4e2a-81bb-23751ad11ac0",
   "metadata": {},
   "source": [
    "<img src=\"docs/ajccbc.png\">\n",
    "<p></p>\n",
    "\n",
    "<h1>Introduction: Enriching data</h1>\n",
    "\n",
    "*by Serge Droz and Eireann Leverett*\n",
    "\n",
    "In this second lessen we will focus on enriching data using external information. These can be of different types, `http`, `DNS` or pretty much anything else. You should be familiar with basic `REST` APIs, and `DNS`.\n",
    "\n",
    "**Note**: This Notebook will not work in `Jupyterlite` \n",
    "\n",
    "#### License\n",
    "\n",
    "This material is available under the [Creative Commons BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.\n",
    "\n",
    "This course includes GeoLite2 Data created by MaxMind, available from\n",
    "[https://www.maxmind.com](https://www.maxmind.com/).\n",
    "\n",
    "## Initialization \n",
    "\n",
    "Please run the cell below by **putting the mouse into the cell** and then **pressing *shift enter***. . First we read in the same data file we used in lesson one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccf832-85fb-4ae5-9b6e-7032e8e09330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "parse_log_line = re.compile( r'([0-9a-f:\\.]+) - ([^\\s]+) \\[([^\\]]+)] \"([^\\\"]*)\" (\\d+) (\\d+) \"([^\\\"]*)\" \"([^\\\"]*)\" \"([^\\\"]*)\"\\s*(\\d*)\\s*([^\\s]*)$'  )\n",
    "def read_logs( filename ):\n",
    "    \"\"\"Read in a ngnix log and split it into fileds.\n",
    "       filname: the name of the logfile\n",
    "       Returns: a data frame containing the log\"\"\"\n",
    "    data = []\n",
    "    with open( filename ) as fp:\n",
    "        for i, line in enumerate(fp.read().split(\"\\n\")):\n",
    "            if not line:\n",
    "                continue\n",
    "            tmp = parse_log_line.findall( line ) \n",
    "            if  len ( tmp ): \n",
    "                data.append( *tmp )\n",
    "            else:\n",
    "                print( \"Error parsing line {:2d}\".format(i))\n",
    "\n",
    "    columns= [ \"IP\", 'User', 'Time', 'Request', 'Status', 'Body Bytes', 'Referer', 'User-Agent', 'Forwarded', 'Dest-Port', 'Host']\n",
    "\n",
    "\n",
    "    df = pd.DataFrame( data, columns=columns )\n",
    "    df['Status']     = df['Status'].apply( lambda x: int(x))\n",
    "    df['Body Bytes'] = df['Body Bytes'].apply( lambda x: int(x))\n",
    "    df['Dest-Port']  = df['Dest-Port'].apply( lambda x:  int(x) if x else -1  )\n",
    "    df['Time'] = pd.to_datetime( df['Time'], format=\"%d/%b/%Y:%H:%M:%S %z\" , utc=True )\n",
    "    df['Time'] = df['Time'].apply( lambda x: x.tz_convert('UTC').tz_localize(None) ) # Convert to UTC\n",
    "    return df\n",
    "df = read_logs( 'data/webserver-log-01.txt' )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e3089-b925-44ff-8d7e-39a2c5b68b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T10:12:52.515466Z",
     "iopub.status.busy": "2022-08-07T10:12:52.515087Z",
     "iopub.status.idle": "2022-08-07T10:12:52.519824Z",
     "shell.execute_reply": "2022-08-07T10:12:52.519478Z",
     "shell.execute_reply.started": "2022-08-07T10:12:52.515446Z"
    },
    "tags": []
   },
   "source": [
    "## DNS\n",
    "\n",
    "Analyzing logfiles very often involves dealing with IP addresses. Thus it's natural to see what domain name belongs to an IP address. \n",
    "The most straight forward way is doing a *reverse lookup*. This is easily done in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0cb71d-62b4-4ab4-b140-f7ce831abed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dns.resolver\n",
    "\n",
    "resolver = dns.resolver.Resolver()\n",
    "\n",
    "def rlookup( ip ):\n",
    "    q =  dns.reversename.from_address( ip )\n",
    "    try:\n",
    "        return resolver.resolve(q, rdtype='PTR').rrset[0].to_text()\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "%time rlookup( '54.175.213.155')\n",
    "%time rlookup( '54.175.213.155')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f17a4ac-238e-499a-9f54-4f21e20e5268",
   "metadata": {},
   "source": [
    "This seems to work. But one thing to keep in mind with network based queries is that they tend to take some time. The magic command `%time` shows you how much. But careful! DNS e.g. may be cached, so a second lookup may be a lot faster. But let's check the situation for DNS with our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af983e6-27b8-4bf9-b2a6-44b5c600e2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list( range( 0, 100, 10 ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2610ac-ac6e-4a95-b198-2d903e8049ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import time\n",
    "\n",
    "def time_it( n ):\n",
    "    start = time.time()\n",
    "    [ rlookup( ip ) for ip in df[:n]['IP'] ]\n",
    "    return time.time() - start\n",
    "\n",
    "seconds = [ time_it( i ) for i in range( 20 ) ]\n",
    "fig, ax = plt.subplots( figsize=(8,5) )\n",
    "\n",
    "ax.bar( range(len(seconds)), seconds )\n",
    "ax.set_xlabel(\"Number of queries\")\n",
    "ax.set_ylabel(\"Seconds\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dde399-4618-47cf-963c-f210c9ce158e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T15:37:22.423422Z",
     "iopub.status.busy": "2022-08-07T15:37:22.423156Z",
     "iopub.status.idle": "2022-08-07T15:37:22.678469Z",
     "shell.execute_reply": "2022-08-07T15:37:22.677898Z",
     "shell.execute_reply.started": "2022-08-07T15:37:22.423392Z"
    },
    "tags": []
   },
   "source": [
    "Obviously this does not work very efficiently if we have to perform many queries. \n",
    "\n",
    "There simple methods to alleviate the problem:\n",
    "\n",
    " - Caching: This works well if we we have have many equal requests\n",
    " - Threading: This is a bit more complicated, but works well if individual queries just need time, i.e. the code is mostly waiting for answers. \n",
    "\n",
    "Let's create an object that does both. The code is a bit involved, so don't worry if it doesn't immediately make sense. \n",
    "\n",
    "Essentially this class provides a function that takes an array of IP addresses and returns a dictionary of IPs pointing to the hostname. Note that there ay be other host name that point to the same IP address.\n",
    "\n",
    "This object already contains code to do DNS blacklist lookups, we';; come to this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504ec0c-1df2-443e-8eec-8aaf6933b99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import dns.resolver\n",
    "\n",
    "class FastResolver( object ):\n",
    "    \n",
    "    def split(self, a, n):\n",
    "        \"\"\"split a list into n sublists\"\"\"\n",
    "        # See https://stackoverflow.com/questions/2130016/splitting-a-list-into-n-parts-of-approximately-equal-length\n",
    "        k, m = divmod(len(a), n)\n",
    "        return list( (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n)) )\n",
    "\n",
    "    def __init__( self, n = 10 ):\n",
    "        \"\"\"Initialize the object. You can specify how many threads you want run concurently\"\"\"\n",
    "        self.n = n \n",
    "        self.resolver = dns.resolver.Resolver()\n",
    "        self.bl_form_query = lambda ip,bl:'.'.join( ( *reversed(ip.split(\".\") ), bl ) )\n",
    "    \n",
    "    def _r_lookup( self, ips, results, i ):\n",
    "        \"\"\"Do an reverse lookup on the ips in the array ips\"\"\"\n",
    "        while ips[i]:\n",
    "            ip = ips[i].pop()\n",
    "            try:\n",
    "                q = dns.reversename.from_address( ip )\n",
    "                results[ip] = self.resolver.resolve(q, rdtype='PTR').rrset[0].to_text()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    def reverse_lookup_list(self, ips):\n",
    "        \"\"\"Do an reverse lookup on the ips in the array ips by running threads\"\"\"\n",
    "        self.ips  = list( set(ips) )\n",
    "        sublists = self.split( self.ips, self.n )\n",
    "        threads = []\n",
    "        results = {}\n",
    "        for index in range(self.n):\n",
    "            x = threading.Thread(target=self._r_lookup, args=( sublists, results, index  ) )\n",
    "            threads.append(x)\n",
    "            x.start()\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "            \n",
    "        #return  { k: d[k] for d in results for k in d.keys() }\n",
    "        return results\n",
    "    \n",
    "    def _bl_lookup( self, ips, results, i, bl ):\n",
    "        \"\"\"Look up a blacklist\"\"\"\n",
    "        while ips[i]:\n",
    "            ip = ips[i].pop()\n",
    "            try:\n",
    "                results[i][ip] = self.resolver.resolve( self.bl_form_query(ip,bl) , 'A')[0].to_text()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def bl_lookup_list(self,ips,bl):\n",
    "        \"\"\"Lookup a an array of ips in a DNS blacklist\"\"\"\n",
    "        self.ips  = list(set(ips))\n",
    "        sublists = self.split( self.ips, self.n )\n",
    "        results = {}\n",
    "        threads = []\n",
    "        for index in range(self.n):\n",
    "            x = threading.Thread(target=self._bl_lookup, args=(sublists, results, index, bl ) )\n",
    "            threads.append(x)\n",
    "            x.start()\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        return results\n",
    "        #return { k: d[k] for d in results for k in d.keys() }\n",
    "    \n",
    "    \n",
    "fresolver = FastResolver()\n",
    "%time r = fresolver.reverse_lookup_list( df[:30]['IP']  )\n",
    "#%time r = fresolver.bl_lookup_list( df[-1000:] , 'zen.spamhaus.org.'  )\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e9d40-aa36-447f-97f1-252cf9a8dbda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_it( n ):\n",
    "    start = time.time()\n",
    "    fresolver.reverse_lookup_list( df[:n]['IP']  )\n",
    "    return time.time() - start\n",
    "\n",
    "seconds = [ time_it( i ) for i in range( 20 ) ]\n",
    "fig, ax = plt.subplots( figsize=(8,5) )\n",
    "\n",
    "ax.bar( range(len(seconds)), seconds )\n",
    "ax.set_xlabel(\"Number of queries\")\n",
    "ax.set_ylabel(\"Seconds\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc396af7-e959-477d-a1dd-285c48ebb7ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-07T17:06:04.176880Z",
     "iopub.status.busy": "2022-08-07T17:06:04.176615Z",
     "iopub.status.idle": "2022-08-07T17:06:04.572332Z",
     "shell.execute_reply": "2022-08-07T17:06:04.571491Z",
     "shell.execute_reply.started": "2022-08-07T17:06:04.176842Z"
    },
    "tags": []
   },
   "source": [
    "This looks much better. This plot may look a bit random. This comes from the fact that DNS itself caches answers. Let's run quickly through the code. For the moment we ignore the functions related to black list lookups (`bl_`)\n",
    "\n",
    "We implemented this as a class. This is good practice for code that may be reused often. We need two python modules:\n",
    "\n",
    " - `dns.resolver` which does the actual DNS lookups\n",
    " - `threading` which is a python interface to [threads](https://docs.python.org/3/library/threading.html)\n",
    " \n",
    "The *constructor* takes one optional argument (besides the `self`), the number of threads. Its set to 10 by default. It then sets up resolver instance. As said, we will ignore the blacklist related stuff later. \n",
    "\n",
    "The function `reverse_lookup_list` takes an array of ip4 addresses, separates it into `n` sub-lists and sets up the treads, which then execute `_r_lookup`. This functions takes the sub-list of IPs as well as a dictionary, `results`. It i an answer is found, it's added as a key value pair to this dictionary. \n",
    "\n",
    "That's it essentially. Threading is not a simple topic, but its beyond this tutorial to explain it, as there are many good explanations out there. \n",
    "\n",
    "This class only handles IPv4 addresses. It can easily be extended to IPv6 addresses though. \n",
    "\n",
    "## Applying this to the available data\n",
    "\n",
    "The above methods returns a dictionary with IP, Names key, value pairs. So we create this dict first and then apply it to the dataframe. Despite the treading, this will take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afe228-69be-4d06-b2d5-a995c140996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time r = fresolver.reverse_lookup_list( df['IP']  )\n",
    "df['DNS'] = df['IP'].apply(lambda x: r[x] if x in r.keys() else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac023e-4c1a-4842-849f-28e95126fa7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T07:28:45.022228Z",
     "iopub.status.busy": "2022-08-23T07:28:45.021680Z",
     "iopub.status.idle": "2022-08-23T07:28:45.038820Z",
     "shell.execute_reply": "2022-08-23T07:28:45.034216Z",
     "shell.execute_reply.started": "2022-08-23T07:28:45.022189Z"
    }
   },
   "source": [
    "Doing a `value_count` you can see that a lot of IPs don't have a reverse lookup configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c853d-3100-4f96-b15a-2f621220225c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['DNS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50519c95-fbf4-44c0-90ca-e3ef0352e073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-23T07:13:02.254369Z",
     "iopub.status.busy": "2022-08-23T07:13:02.253613Z",
     "iopub.status.idle": "2022-08-23T07:13:02.728080Z",
     "shell.execute_reply": "2022-08-23T07:13:02.725644Z",
     "shell.execute_reply.started": "2022-08-23T07:13:02.254294Z"
    }
   },
   "source": [
    "## Leveraging DNS \n",
    "\n",
    "We can now start looking at the returned DNS names. Remember, that in the fist lesson we tried to identify scanners from their User-Agent strings. Some scanners, especially if they scan ports other than 80 or 443, have this info in their DNS names. Let's see if we find such scanner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c1fdb-ec9e-4139-983d-e6f6633e1271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ua_scanner_keywords = [ 'project_patchwatch', 'scaninfo@expanseinc.com', 'project-resonance.com' ,\n",
    "                            'gdnplus.com', 'censys.io', 'netsystemsresearch.com', 'Nmap Scripting Engine'\n",
    "                            'internet-measurement.com', 'InternetMeasurement', 'tchelebi.io', 'paloaltonetworks.com', 'security.ipip.net' ] \n",
    "dns_scanner_keywords = ['shodan.io', 'research-scanner.com', 'censys-scanner.com', 'shadowserver.org', 'the-shadow-server-foundation', 'internet-measurement.com', 'internet-census.org', 'netsystemsresearch.com', 'leakix.org', 'stretchoid.com', 'recyber.net', 'planetlab', 'criminalip.com', 'quadmetrics.com', 'scanopticon.com', 'onyphe.net', 'crd.bi.zone', 'michael-kipp.com', 'mapletoast.com', 'maaketing.nl', 'intenettl.org', 'iostation.io', 'intrinsec.com', 'security.ipip.net', 'rnd.group-ib.com', 'threatsinkhole.com', 'binaryedge.ninja']\n",
    "\n",
    "def is_scanner( x ):\n",
    "    \n",
    "    tmp = []\n",
    "    for kw in dns_scanner_keywords:\n",
    "        if kw in x['DNS']:\n",
    "            tmp = ['DNS']\n",
    "            break\n",
    "    for kw in ua_scanner_keywords:\n",
    "        if kw in x['User-Agent']:\n",
    "            tmp.append( \"UA\" )\n",
    "\n",
    "    return ','.join(tmp)\n",
    "\n",
    "df['Scanner'] = df.apply( is_scanner, axis = 1 )\n",
    "\n",
    "ax = df['Scanner'].value_counts().plot(kind=\"pie\",figsize=(6,6), fontsize=12)\n",
    "_ = ax.set_ylabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c591a1-c3cf-464d-beb5-9fcdb275ed49",
   "metadata": {},
   "source": [
    "A lot more IPs scan than the User-Agent analysis may make you believe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2a057-1b35-456c-b3ef-09787b1dda97",
   "metadata": {},
   "source": [
    "## DNS Blocklists\n",
    "\n",
    "Often IPs are already known to be malicious. A great source of such info are DNS blocklists (DNSBL), although  the name blocklist is missleading. The decission of what to do with an IP is independent of it being available. \n",
    "\n",
    "DNS has the advantageof being fast. DNSBL all seem to use the same mechanisms. For an IPv4 A.B.C.D it works essentially is a A record lookup for D.C.B.A.some.domain, where some.domain is specific for the blacklist used. E.g. for Spamhaus' ZEN list it would be `zen.spamhaus.org.`. The replies are IP addresses, by convention all in `127.0.0.0/8`. Depending on the blacklist different answers are returned indication the abuse. Again for ZEN these are\n",
    "\n",
    "```\n",
    "        \"127.0.0.2\" : \"SBL Spamhaus SBL Data\",\n",
    "        \"127.0.0.3\" : \"SBL Spamhaus SBL CSS Data\",\n",
    "        \"127.0.0.4\" : \"XBL CBL Data\",\n",
    "        \"127.0.0.9\" : \"SBL Spamhaus DROP/EDROP Data\",\n",
    "        \"127.0.0.10\" : \"PBL ISP Maintained\",\n",
    "        \"127.0.0.11\" : \"PBL Spamhaus Maintained\",\n",
    "        \"127.255.255.254\" : 'Error'\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write a function that takes an IPv4 address and queries ZEN, returning the result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f424630-8775-4688-8947-a3389c821b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here and test a few of the addresses in the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc317e1-6cd9-46b0-b022-569a9eec07bf",
   "metadata": {},
   "source": [
    "The above DNS class has already support for queying DNSBLs.\n",
    "Ideally a dataframe is updated with the reason as well as the return code. \n",
    "\n",
    "## Other sources\n",
    "\n",
    "There are a variety of others free TI sources, such as MISP instances, OTX, Cymru Whois etc.\n",
    "Some have their own Python modules others, such as the Cymru Whois service need custom code. \n",
    "This really is not much different from what we did so far. \n",
    "\n",
    "However many ressources have rate limits, so remember to cache and only query specifi info you ight need. \n",
    "\n",
    "# Geographic information\n",
    "\n",
    "Often it is interesting to see where IPs are coming from. A cool way to do this is using MaxMinds free [GeoLite2](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data) data set and [folium](http://python-visualization.github.io/folium/), a python interface to [leaflet](https://leafletjs.com) allowing one to create interactive maps. By default folium/leaflet use openstreetmap.\n",
    "\n",
    "MaxMind offers an API, but it is much slower than a local lookup, and it does not disclose the IPs you are looking at. \n",
    "\n",
    "While the code below may look complicated it's actually quite simple. We first define a function that retruns a color for a given value between 0 and 1. \n",
    "\n",
    "Then we set up the geoip and map objects. What we really want to plot though is the value counts of the ips. \n",
    "\n",
    "The lines\n",
    "```\n",
    "tmp = df['IP'].value_counts()\n",
    "values = list(zip(tmp.index.tolist(), tmp.values.tolist() ) )\n",
    "```\n",
    "\n",
    "create a list of ip count pairs out of the pandas Series. The rest is really pretty straight forward: Loop over the entries and plot a colored dot at the coordinates of each IP. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b268c-a714-4b7e-80bb-3a74ac357285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import geoip2.database\n",
    "import folium\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "#cmap = matplotlib.cm.get_cmap('YlOrRd')\n",
    "cmap = matplotlib.colormaps['YlOrRd']\n",
    "def get_color( v ):\n",
    "    '''Return a palette RGB value for an argumennt between 0 and 1.'''\n",
    "    (r, g, b) = [ int(255 * i) for i in  cmap(1-v)[:-1] ]\n",
    "    #print( r,g,b )\n",
    "    return '#{:06X}'.format( 256 * ( 256 * r + g ) + b )  \n",
    "\n",
    "reader = geoip2.database.Reader( os.path.expanduser('data/GeoLite2-City_20240611/GeoLite2-City.mmdb') )\n",
    "m = folium.Map(zoom_start=2, location=[0,0])\n",
    "# \n",
    "tmp = df['IP'].value_counts()\n",
    "values = list(zip(tmp.index.tolist(), tmp.values.tolist() ) )\n",
    "m_value = values[0][1]\n",
    "\n",
    "for i in values:\n",
    "    try:\n",
    "        response = reader.city( i[0] )\n",
    "    except:\n",
    "        continue\n",
    "    c = get_color( math.log( i[1] ) / math.log( m_value) )\n",
    "    pos = ( response.location.latitude , response.location.longitude )\n",
    "    #folium.Marker( location=pos,icon=folium.Icon(color='darkpurple') ).add_to(m)\n",
    "    try:\n",
    "        folium.CircleMarker( pos , radius=2, color=c ).add_to(m)\n",
    "    except Exception as e:\n",
    "        print(e, i, pos)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d4fe9d-9ce2-41c2-a254-6efed9954540",
   "metadata": {},
   "source": [
    "The result, while looking cool is hard to read. It's much moch more interesting to combine, or cluster, points nearby. Folium can do that fo you, check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f6e40-4c03-4115-a576-7228cfe7eba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "\n",
    "#cmap = matplotlib.cm.get_cmap('YlOrRd')\n",
    "cmap = matplotlib.colormaps['YlOrRd']\n",
    "def get_color( v ):\n",
    "    '''Return a palette RGB value for an argumennt between 0 and 1.'''\n",
    "    (r, g, b) = [ int(255 * i) for i in  cmap(1-v)[:-1] ]\n",
    "    #print( r,g,b )\n",
    "    return '#{:06X}'.format( 256 * ( 256 * r + g ) + b )  \n",
    "\n",
    "reader = geoip2.database.Reader( os.path.expanduser('data/GeoLite2-City_20240611/GeoLite2-City.mmdb') )\n",
    "\n",
    "tmp = df['IP'].value_counts()\n",
    "\n",
    "values = list(zip(tmp.index.tolist(), tmp.values.tolist() ) )\n",
    "\n",
    "m = folium.Map(zoom_start=2, location=[0,0])\n",
    "m_value = values[0][1]\n",
    "\n",
    "marker_cluster = MarkerCluster()\n",
    "\n",
    "\n",
    "for i in values:\n",
    "    try:\n",
    "        response = reader.city( i[0] )\n",
    "    except:\n",
    "        continue\n",
    "    c = get_color( math.log( i[1] ) / math.log( m_value) )\n",
    "    pos = ( response.location.latitude , response.location.longitude )\n",
    "    try:\n",
    "        folium.CircleMarker( pos , radius=5, color=c, fill=True, fill_opacity=1 ).add_to(marker_cluster)\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "marker_cluster.add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e9fae-632e-4e6b-8534-e18667797d21",
   "metadata": {},
   "source": [
    "These maps are only viewable in a webbrowser. But you might need classic pixmaps.  This can be achived with a little workaround: Rendering the map in a virtual browser and taking a screenshot. Note, that this needs [selenium](https://selenium-python.readthedocs.io/) to be installed. The croping needs to be finetuned a bit by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb78073-bb5c-4373-a57d-db46a2377693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import os \n",
    "import logging\n",
    "\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/opt/conda/lib/\"\n",
    "os.environ[\"SE_MANAGER_PATH\"] = \"/opt/conda/bin/selenium-manager\"\n",
    "\n",
    "def map_to_png( m ):\n",
    "    img_data = m._to_png(5)\n",
    "    # crop( ( left, upper, right, and lower ))\n",
    "    return Image.open(io.BytesIO(img_data)).crop( (170,20, 1195,600))\n",
    "    #img.save('image.png')\n",
    "display(map_to_png(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
